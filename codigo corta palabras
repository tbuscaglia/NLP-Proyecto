from nltk.tokenize import word_tokenize
import nltk
nltk.download('punkt')

import csv
words = []
with open('/Users/julianandelsman/Desktop/NLP/Final project/Clean data/avfc_commentsfiltered.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter='\t')
    line_count = 0
    for row in csv_reader:
        if line_count > 0:
            words.extend(row[1].lower().split(' '))
        line_count += 1
word_length = []
for w in words:
  word_length.append(len(w))
import numpy as np
mean_len = np.mean(word_length)
std_len = np.std(word_length)

sentences = []
with open('/Users/julianandelsman/Desktop/NLP/Final project/Clean data/avfc_commentsfiltered.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter='\t')
    line_count = 0
    for row in csv_reader:
        if line_count > 0:
            words = row[1].lower().split(' ')
            valid_words = []
            for w in words:
              if (len(w)-mean_len)/std_len <3:
                valid_words.append(w)
            sentences.append(' '.join(valid_words))
        line_count += 1
